library(mice)
library(GGally)
library(rpart)
library(randomForest)
library (missForest)
library(VIM)
library(klaR)
library(e1071)
library(caTools)
library(MASS)
library(caret)

##Import the data set
#STep 2 : Reading data into R
data<- read.csv("C:/Users/norazyrabinti.omar/Documents/MS DS/10. Machine Learning for Data Science/icu.csv")

#Setting outcome variables as categorical
data$ICU <- factor(data$ICU, levels = c(0,1), labels = c("False", "True"))
data$ICU

#Studying the Data Set
#Step 3 : Studying the structure of the data
str(data)
head(data)
#describe(data)

#Step 4: Data Cleaning
is.na(data)

##mean imputation of multiple columns (i.e. the whole data frame) #####

for(i in 1:ncol(data)) {
  data[ , i][is.na(data[ , i])] <- mean(data[ , i], na.rm = TRUE)
}
head(data) # Check first 6 rows after substitution by mean

#Step 5 : Data Modelling
# Splitting data into train
# and test data
split <- sample.split(data, SplitRatio = 0.7)
train_cl <- subset(data, split == "TRUE")
test_cl <- subset(data, split == "FALSE")

# Fitting Naive Bayes Model
# to training dataset
NB_model <- naiveBayes(ICU ~ ., data = train_cl)

# Predicting on test data'
y_pred <- predict(NB_model, newdata = test_cl)

#Step 6 : Model Evaluation
# Confusion Matrix
cm <- table(test_cl$ICU, y_pred)
cm<-as.matrix(cm)
cm

# Model Evaluation
confusionMatrix(cm)

false_positives<-cm[2,1]
false_positives
false_negatives<-cm[1,1]
false_negatives

true_positives<-cm[2,2]
true_positives
true_negatives<-cm[1,2]
true_negatives

precision <-true_positives/ (true_positives + false_positives)
precision

recall <-true_positives/ (true_positives + false_negatives)
recall

Fmeasure <- (2*precision*recall) / (precision + recall)
Fmeasure
